# -------------------------------------------------------------------------
# AUTHOR: Miro Abdalian
# FILENAME: title of the source file
# SPECIFICATION: description of the program
# FOR: CS 4210- Assignment #3
# TIME SPENT: 1 hour and half
# -----------------------------------------------------------*/

# IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas. You have to work here only with standard vectors and arrays

# importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import csv

dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = []  # this array will be used to count the votes of each classifier

# reading the training data from a csv file and populate dbTraining
with open('./HW3-3/optdigits.tra', 'r') as csvfile:
   reader = csv.reader(csvfile)
   for i, row in enumerate(reader):
      dbTraining.append(row)

# reading the test data from a csv file and populate dbTest
with open('./HW3-3/optdigits.tes', 'r') as csvfile:
   reader = csv.reader(csvfile)
   for i, row in enumerate(reader):
      dbTest.append(row)

# inititalizing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
for row in dbTest:
   classVotes.append([0]*10)

print("Started my base and ensemble classifier ...")

for k in range(20):  # we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample
   bootstrapSample = resample(
      dbTraining, n_samples=len(dbTraining), replace=True)

    # populate the values of X_training and y_training by using the bootstrapSample
   for row in bootstrapSample:
      X_training.append(row[:-1])
      y_training.append(row[-1])

    # fitting the decision tree to the data
    # we will use a single decision tree without pruning it
   clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)
   clf = clf.fit(X_training, y_training)

   counter = 0
   for i, testSample in enumerate(dbTest):

      # make the classifier prediction for each test sample and update the corresponding index value in classVotes. For instance,
      # if your first base classifier predicted 2 for the first test sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
      # Later, if your second base classifier predicted 3 for the first test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
      # Later, if your third base classifier predicted 3 for the first test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
      # this array will consolidate the votes of all classifier for all test samples
      class_predicted = clf.predict([testSample[:-1]])[0]

      classVotes[i][int(class_predicted)] += 1

      if k == 0:  # for only the first base classifier, compare the prediction with the true label of the test sample here to start calculating its accuracy
         if class_predicted == testSample[-1]:
            counter += 1

   if k == 0:  # for only the first base classifier, print its accuracy here
      accuracy = counter/len(dbTest)
      print("Finished my base classifier (fast but relatively low accuracy) ...")
      print(f"My base classifier accuracy: {accuracy:.2%}")
      print("")

# now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)

# ensemble_predicted = list(map(lambda votes: max(votes), classVotes))
ensemble_predicted = list(map(lambda votes: [index for index,\
   vote in enumerate(votes) if vote == max(votes)][0], classVotes))
  
counter = 0
for i in range(len(ensemble_predicted)):
   if ensemble_predicted[i] == int(dbTest[i][-1]):
      counter += 1
      
# printing the ensemble accuracy here
accuracy = counter / len(dbTest)
print("Finished my ensemble classifier (slow but higher accuracy) ...")
print(f"My ensemble accuracy: {accuracy:.2%}")
print("")

print("Started Random Forest algorithm ...")

# Create a Random Forest Classifier
clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

# Fit Random Forest to the training data
X_training.clear()
y_training.clear()
for row in dbTraining:
   X_training.append(row[:-1])
   y_training.append(row[-1])
   
clf.fit(X_training,y_training)

# make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
# compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
counter = 0
for row in dbTest:
   class_predicted_rf = clf.predict([row[:-1]])
   if class_predicted_rf == row[-1]:
      counter += 1

# printing Random Forest accuracy here
accuracy = counter / len(dbTest)
print(f"Random Forest accuracy: {accuracy:.2%}")

print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
